# Import necessary libraries
from datasets import load_dataset, load_metric
from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding, pipeline, Trainer, TrainingArguments
from torch.utils.data import DataLoader
import torch
from peft import LoraConfig, get_peft_model, AutoPeftModelForSequenceClassification

# Load a dataset from the Hugging Face Hub
dataset_name = "imdb"
dataset = load_dataset(dataset_name, split="train[:10%]")  # Load a subset (10%) of the training data to reduce computational resources

# Define the model and tokenizer name
model_name = "distilbert-base-uncased-finetuned-sst-2-english"

# Load the pretrained tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Tokenize the dataset
def tokenize_function(examples):
    return tokenizer(examples["text"], padding="max_length", truncation=True)

tokenized_dataset = dataset.map(tokenize_function, batched=True)

# Set the format to return PyTorch tensors
tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])

# Prepare the dataset for PyTorch
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
data_loader = DataLoader(tokenized_dataset, batch_size=8, collate_fn=data_collator)

# Load the pretrained model
base_model = AutoModelForSequenceClassification.from_pretrained(model_name)

# Print the model architecture before fine-tuning
print("Model architecture before fine-tuning:")
print(base_model)

# Load the metric
metric = load_metric("accuracy")

# Function to compute metrics
def compute_metrics(predictions, references):
    return metric.compute(predictions=predictions, references=references)

# Evaluate the base model on the dataset
print("\nEvaluating the base model...")
base_predictions = []
base_references = []

for batch in data_loader:
    inputs = {key: value for key, value in batch.items() if key != 'text'}
    with torch.no_grad():
        outputs = base_model(**inputs)
    logits = outputs.logits
    preds = torch.argmax(logits, dim=-1).numpy()
    labels = inputs['labels'].numpy()
    base_predictions.extend(preds)
    base_references.extend(labels)

# Compute accuracy for the base model
base_accuracy = compute_metrics(base_predictions, base_references)

# Print the accuracy of the base model
print(f"\nBase Model Accuracy: {base_accuracy['accuracy']:.4f}")

# Initialize PEFT configuration and model
peft_config = LoraConfig(
    r=4,
    lora_alpha=16,
    lora_dropout=0.1,
    target_modules=["attention.q_lin", "attention.v_lin"]  # Adjusted for DistilBERT architecture
)

peft_model = get_peft_model(base_model, peft_config)

# Print the model architecture after applying PEFT
print("\nModel architecture after applying PEFT:")
print(peft_model)

# Define training arguments
training_args = TrainingArguments(
    output_dir='./results',
    evaluation_strategy="epoch",
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3,
    weight_decay=0.01,
    logging_dir='./logs',
)

# Initialize the Trainer
trainer = Trainer(
    model=peft_model,
    args=training_args,
    train_dataset=tokenized_dataset,
    eval_dataset=tokenized_dataset,
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=lambda p: compute_metrics(p.predictions.argmax(-1), p.label_ids)
)

# Fine-tune the model
trainer.train()

# Save the PEFT model
peft_model.save_pretrained('./peft_model')

# Load the saved PEFT model for inference
loaded_peft_model = AutoPeftModelForSequenceClassification.from_pretrained('./peft_model')

# Evaluate the PEFT model on the dataset after fine-tuning
print("\nEvaluating the PEFT model after fine-tuning...")
peft_predictions = []
peft_references = []

for batch in data_loader:
    inputs = {key: value for key, value in batch.items() if key != 'text'}
    with torch.no_grad():
        outputs = loaded_peft_model(**inputs)
    logits = outputs.logits
    preds = torch.argmax(logits, dim=-1).numpy()
    labels = inputs['labels'].numpy()
    peft_predictions.extend(preds)
    peft_references.extend(labels)

# Compute accuracy for the PEFT model
peft_accuracy = compute_metrics(peft_predictions, peft_references)

# Print the accuracy of the PEFT model after fine-tuning
print(f"\nPEFT Model Accuracy after fine-tuning: {peft_accuracy['accuracy']:.4f}")

# Test the classifier with an example sentence using the loaded PEFT model
example_sentence = "I love using Hugging Face models for NLP tasks!"
classifier = pipeline("sentiment-analysis", model=loaded_peft_model, tokenizer=tokenizer)
result = classifier(example_sentence)

# Print the result
print("\nSentiment analysis result for the example sentence:")
print(result)
